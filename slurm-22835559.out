Found 9 chunk files. Aggregating results...
Deleted: results/mcq_results/relevant/mmlu/Mistral-7B-Instruct-v0.3_26.csv
Deleted: results/mcq_results/relevant/mmlu/Mistral-7B-Instruct-v0.3_23.csv
Deleted: results/mcq_results/relevant/mmlu/Mistral-7B-Instruct-v0.3_27.csv
Deleted: results/mcq_results/relevant/mmlu/Mistral-7B-Instruct-v0.3_28.csv
Deleted: results/mcq_results/relevant/mmlu/Mistral-7B-Instruct-v0.3_29.csv
Deleted: results/mcq_results/relevant/mmlu/Mistral-7B-Instruct-v0.3_24.csv
Deleted: results/mcq_results/relevant/mmlu/Mistral-7B-Instruct-v0.3_22.csv
Deleted: results/mcq_results/relevant/mmlu/Mistral-7B-Instruct-v0.3_21.csv
Deleted: results/mcq_results/relevant/mmlu/Mistral-7B-Instruct-v0.3_25.csv
profile_0_answer_accuracy: 45.22%
profile_1_answer_accuracy: 43.20%
profile_2_answer_accuracy: 40.08%
profile_3_answer_accuracy: 36.96%
profile_4_answer_accuracy: 39.75%
profile_5_answer_accuracy: 40.21%
profile_6_answer_accuracy: 29.47%
profile_7_answer_accuracy: 41.51%
profile_8_answer_accuracy: 35.26%
profile_9_answer_accuracy: 38.06%
profile_10_answer_accuracy: 37.67%
profile_11_answer_accuracy: 42.23%
lenght of final df is 1537
Aggregated results saved at: results/mcq_results/relevant/mmlu/Mistral-7B-Instruct-v0.3_final.csv
profile_0_answer_accuracy: 47.70%
profile_1_answer_accuracy: 44.66%
profile_2_answer_accuracy: 41.37%
profile_3_answer_accuracy: 40.02%
profile_4_answer_accuracy: 43.85%
profile_5_answer_accuracy: 41.72%
profile_6_answer_accuracy: 36.46%
profile_7_answer_accuracy: 43.97%
profile_8_answer_accuracy: 41.59%
profile_9_answer_accuracy: 41.78%
profile_10_answer_accuracy: 43.23%
profile_11_answer_accuracy: 44.33%
